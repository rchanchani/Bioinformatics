---
title: "Unsupervised Learning Analysis of Cancer Cells"
author: "Raghav Chanchani"
date: "10/30/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preparing Data

We use the Wisconsin Breast Cancer Diagnostic Data Set which show provide information from digitized images of a fine needle aspiration (FNA) of a breast mass. The data provides information including radius (mean of distances from center to points on the perimeter), and smoothness (local variation in radius lengths) and malignant or benign diagnosis.

Read in the dataset directly from the url, read the csv file as a dataframe, convert the dataframe into a matrix and only view the second to the 32nd columns for all patients. Assign the ID as the row names of the matrix.
```{r prepare}
url <- "https://bioboot.github.io/bimm143_W18/class-material/WisconsinCancer.csv"
wisc.df <- read.csv(url)
wisc.data <- as.matrix( wisc.df[,3:32] )
```

We change the "M" and "B" markers for malignant and benign diagnoses to be logical 1 and 0 values respectively.
```{r diagnosis}
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```

There are `r nrow(wisc.df)` patients in the dataset and `r length(grep("_mean", colnames(wisc.data)))` features collected for each of them that are mean values. There are `r sum(diagnosis)` malignant diagnoses.

## Principal Component Analysis

```{r mean, echo=FALSE}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

Using principal component analysis we see the contribution of each variable on the variance of the dataset.
```{r pr}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

Plotting two significant principal components with the most significant feature on the x-axis.
```{r biplot, echo=FALSE}
biplot(wisc.pr)

plot( wisc.pr$x[, c(1,2)] , col = (diagnosis + 1) , 
     xlab = "PC1", ylab = "PC2")

plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```
The second PC1 vs. PC2 has a demarcation between diagnoses that is more well-defined than that between PC1 vs. PC3, so the first plot is more useful in clustering the data.

## Evaluating Variance
Scree plots that show the contribution of each of the principal components on the variance of the dataset.
```{r pve, echo=FALSE}
pr.vr <- wisc.pr$sdev^2
pve <- pr.vr / sum(pr.vr)

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )

plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
#plot( cumsum() , xlab = "Principal Component", 
#     ylab = "Cumulative Proportion of Variance Explained", 
#     ylim = c(0, 1), type = "o")
```

The component of the loading vector for concave.points_mean is `r #wisc.pr$rotation[,1]$concave.points_mean`. `#r ` principal components are required to explain 80% of the variance of the data.

## Hierarchical Clustering

```{r scale.hclust}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method="complete")
plot(wisc.hclust)
```
```{r num.clust}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```

## K-means Clustering

```{r}
wisc.km <- kmeans(data.scaled, centers= 4, nstart= 100)
```

## Clustering on PCA Results

```{r}
wisc.pr.dist <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(wisc.pr.dist, method="complete")
plot(wisc.pr.hclust)
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)

table(wisc.pr.hclust.clusters, diagnosis)

```

## Predicting Malignancy Of New Samples

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)

plot(wisc.pr$x[,1:2], col=(diagnosis+1))
points(npc[,1], npc[,2], col="blue", pch=16)
```